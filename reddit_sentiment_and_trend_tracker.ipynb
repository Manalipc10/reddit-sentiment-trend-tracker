{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2329d9b1-f76b-47b7-b396-e4024dd71150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\manal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from keybert import KeyBERT\n",
    "import numpy as np\n",
    "from prefect import flow, task\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9fa12-84b2-41aa-a276-b25881217fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fetch Data from Reddit\n",
    "@task\n",
    "def fetch_reddit_posts(subreddit=\"technology\", limit=50):\n",
    "    url = f\"https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&size={limit}&sort=desc\"\n",
    "    res = requests.get(url)\n",
    "    data = res.json().get(\"data\", [])\n",
    "\n",
    "    if not data:\n",
    "        print(\" No posts fetched from Pushshift.\")\n",
    "        return pd.DataFrame(columns=[\"title\", \"selftext\", \"created_utc\", \"subreddit\", \"score\"])\n",
    "\n",
    "    posts = []\n",
    "    for d in data:\n",
    "        title = d.get(\"title\") or d.get(\"selftext\") or \"(no title)\"\n",
    "        posts.append({\n",
    "            \"id\": d.get(\"id\"),\n",
    "            \"title\": str(title).strip(),\n",
    "            \"selftext\": str(d.get(\"selftext\", \"\")),\n",
    "            \"created_utc\": datetime.utcfromtimestamp(d.get(\"created_utc\", 0)),\n",
    "            \"subreddit\": d.get(\"subreddit\", subreddit),\n",
    "            \"score\": d.get(\"score\", 0)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(posts)\n",
    "    if \"title\" not in df.columns:\n",
    "        df[\"title\"] = \"(missing)\"\n",
    "    print(\"Columns fetched:\", df.columns.tolist())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820f498-e1a9-4b1b-80f8-1d25d704967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Sentiment Analysis\n",
    "@task\n",
    "def analyze_sentiment(df):\n",
    "    if \"title\" not in df.columns:\n",
    "        print(\"No title column, creating empty one.\")\n",
    "        df[\"title\"] = \"\"\n",
    "\n",
    "    def get_sentiment(text):\n",
    "        try:\n",
    "            return TextBlob(text).sentiment.polarity\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    df[\"sentiment_score\"] = df[\"title\"].apply(get_sentiment)\n",
    "    df[\"sentiment_label\"] = df[\"sentiment_score\"].apply(\n",
    "        lambda x: \"positive\" if x > 0.1 else (\"negative\" if x < -0.1 else \"neutral\")\n",
    "    )\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac56680-977e-402e-b7df-fb98957752cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic clustering\n",
    "@task\n",
    "def cluster_topics(df, num_clusters=5):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "    embeddings = model.encode(df[\"title\"].tolist(), show_progress_bar=False)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    df[\"cluster\"] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    df[\"topic_keywords\"] = df.groupby(\"cluster\")[\"title\"].transform(\n",
    "        lambda x: \", \".join([kw[0] for kw in kw_model.extract_keywords(\" \".join(x), top_n=3)])\n",
    "    )\n",
    "    print(\" Topic clustering complete.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3506e-977a-465a-b45b-959641822609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store csv\n",
    "@task\n",
    "def store_to_csv(df):\n",
    "    filename = f\"reddit_sentiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\" Saved results to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87c93d-d520-4489-804f-b79ab992817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store to sqlite\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "@task\n",
    "def store_to_sqlite(df):\n",
    "    \"\"\"Store the final DataFrame in a local SQLite database.\"\"\"\n",
    "    engine = create_engine(DB_PATH)\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS reddit_sentiment\"))\n",
    "        print(\"Dropped old table to refresh schema.\")\n",
    "    df.to_sql(\"reddit_sentiment\", engine, if_exists=\"replace\", index=False)\n",
    "    print(\"Saved results to SQLite database (table: reddit_sentiment).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141b52f-7209-4704-a5c6-7866fdbd223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-19 20:05:36.949545\n",
      "2025-10-20 00:05:36.952499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manal\\AppData\\Local\\Temp\\ipykernel_21344\\3520713580.py:5: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  print(datetime.utcfromtimestamp(time.time()))  # âœ… should print UTC version\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(datetime.now()) \n",
    "print(datetime.utcfromtimestamp(time.time())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948ca6a-b575-43aa-a0a9-246b90a2f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define flow\n",
    "import requests\n",
    "\n",
    "@task\n",
    "def get_hot_subreddits(limit=5):\n",
    "    \"\"\"Fetch the top trending subreddits automatically.\"\"\"\n",
    "    url = \"https://www.reddit.com/subreddits/popular.json?limit=50\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, timeout=15)\n",
    "        res.raise_for_status()\n",
    "        data = res.json()[\"data\"][\"children\"]\n",
    "        sorted_subs = sorted(data, key=lambda x: x[\"data\"][\"subscribers\"], reverse=True)\n",
    "        top_subs = [x[\"data\"][\"display_name\"].lower() for x in sorted_subs[:limit]]\n",
    "        print(f\"Top {limit} subreddits detected: {top_subs}\")\n",
    "        return top_subs\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching trending subreddits:\", e)\n",
    "        # fallback default list\n",
    "        return [\"technology\", \"ai\", \"worldnews\", \"science\", \"gaming\"]\n",
    "\n",
    "\n",
    "@flow(name=\"Reddit Sentiment Tracker - Topic Aware\")\n",
    "def reddit_sentiment_flow(limit=50):\n",
    "    \"\"\"Main Prefect flow to process top trending subreddits.\"\"\"\n",
    "    subreddits = get_hot_subreddits()\n",
    "    all_data = []\n",
    "\n",
    "    for sub in subreddits:\n",
    "        df = fetch_reddit_posts(subreddit=sub, limit=limit)\n",
    "        df = analyze_sentiment(df)\n",
    "        df = cluster_topics(df)\n",
    "        all_data.append(df)\n",
    "\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        store_to_csv(combined_df)\n",
    "        store_to_sqlite(combined_df)\n",
    "        print(f\"Processed {len(combined_df)} total posts from {len(subreddits)} subreddits.\")\n",
    "    else:\n",
    "        print(\"No data fetched from any subreddit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4fbdb-9237-450b-9521-5f200e4e0918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:05:57.297 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'meaty-petrel'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'meaty-petrel'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'Reddit Sentiment Tracker - Topic Aware'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:05:57.297 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'meaty-petrel'\u001b[0m - Beginning flow run\u001b[35m 'meaty-petrel'\u001b[0m for flow\u001b[1;35m 'Reddit Sentiment Tracker - Topic Aware'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:05:57.306 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'meaty-petrel'</span> - View at <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://app.prefect.cloud/account/88b7615e-b17e-4a76-bf06-adef7643bce8/workspace/52add0de-7fac-441b-a36b-9bfa0f3f50dc/runs/flow-run/068f57d1-5697-7b2d-8000-1417e69a385e</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:05:57.306 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'meaty-petrel'\u001b[0m - View at \u001b[94mhttps://app.prefect.cloud/account/88b7615e-b17e-4a76-bf06-adef7643bce8/workspace/52add0de-7fac-441b-a36b-9bfa0f3f50dc/runs/flow-run/068f57d1-5697-7b2d-8000-1417e69a385e\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:05:58.914 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'fetch_reddit_posts-79a' - Task run failed with exception: AttributeError(\"type object 'datetime.datetime' has no attribute 'datetime'\") - Retry 1/3 will start 10 second(s) from now\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:05:58.914 | \u001b[36mINFO\u001b[0m    | Task run 'fetch_reddit_posts-79a' - Task run failed with exception: AttributeError(\"type object 'datetime.datetime' has no attribute 'datetime'\") - Retry 1/3 will start 10 second(s) from now\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#run flow\n",
    "if __name__ == \"__main__\":\n",
    "    reddit_sentiment_flow(limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e11fd-108d-4370-9c6d-2fec6f392efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///reddit_sentiment.db\")\n",
    "df = pd.read_sql(\"SELECT * FROM reddit_sentiment\", engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18e13d-9652-4d51-be2a-e00c47c41508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reddit-env)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
